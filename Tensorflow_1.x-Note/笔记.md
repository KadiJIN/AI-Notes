# Tensorflow
TensorFlow, as the name indicates, is a framework to define and run computations involving tensors. 

## TensorFlow Architecture

![Architecture](https://camo.githubusercontent.com/7dc69e6be23b1c0ca70e73f7a625ad93c05319ca/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f6c61796572732e706e67)
## High-level API

### Keras
Keras is a high-level API to build and train deep learning models. It's used for fast prototyping, advanced research, and production. 
####  Build a simple model
##### Sequential Model
##### Configure the layers
There are many tf.keras.layers available with some common constructor parameters:
+ activation: Set the activation function for the layer. This parameter is specified by the name of a built-in function or as a callable object. By default, no activation is applied.
+ kernel_initializer and bias_initializer: The initialization schemes that create the layer's weights (kernel and bias). This parameter is a name or a callable object. This defaults to the "Glorot uniform" initializer.
+ kernel_regularizer and bias_regularizer: The regularization schemes that apply the layer's weights (kernel and bias), such as L1 or L2 regularization. By default, no regularization is applied.
#### Train and evaluate
##### Set up training
After the model is constructed, configure its learning process by calling the compile method.
tf.keras.Model.compile takes three important arguments:
+ optimizer: This object specifies the training procedure. Pass it optimizer instances from the tf.train module, such as tf.train.AdamOptimizer, tf.train.RMSPropOptimizer, or tf.train.GradientDescentOptimizer.
+ loss: The function to minimize during optimization. Common choices include mean square error (mse), categorical_crossentropy, and binary_crossentropy. Loss functions are specified by name or by passing a callable object from the tf.keras.losses module.
+ metrics: Used to monitor training. These are string names or callables from the tf.keras.metrics module.
##### Input data(NumPy or tf.data datasets)
tf.keras.Model.fit takes three important arguments:
+ epochs: Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches).
+ batch_size: When passed NumPy data, the model slices the data into smaller batches and iterates over these batches during training. This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size.
+ validation_data: When prototyping a model, you want to easily monitor its performance on some validation data. Passing this argument—a tuple of inputs and labels—allows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch.
##### Evaluate and predict
The tf.keras.Model.evaluate and tf.keras.Model.predict methods can use NumPy data and a tf.data.Dataset.
#### Build advanced models

### Eager Execution

### Importing Data

## Estimators

## Accelerators

## Low-level API

### Tensors
A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.  
A tf.Tensor has the following properties:  a data type (float32, int32, or string, for example)  + a shape
#### Rank
The rank of a tf.Tensor object is its number of dimensions.  
Rank 0 = 标量， Rank 1 = 向量， Rank 2 = 矩阵， Rank n = n维Tensor
#### Shape
The shape of a tensor is the number of elements in each dimension.

Rank | Shape | Dimension number | Example
--- | --- | --- | ---
0 | [] | 0-D | A 0-D tensor.  A scalar.
1 | [D0] | 1-D | A 1-D tensor with shape [5].
2 | [D0, D1] | 2-D | A 2-D tensor with shape [3, 4].
3 | [D0, D1, D2] | 3-D | A 3-D tensor with shape [1, 4, 3].
n | [D0, D1, ... Dn-1] | n-D | A tensor with shape [D0, D1, ... Dn-1].

#### Getting a tf.Tensor object's shape
#### Changing the shape of a tf.Tensor
#### Data types
When creating a tf.Tensor from a python object you may optionally specify the datatype. If you don't, TensorFlow chooses a datatype that can represent your data. TensorFlow converts Python integers to tf.int32 and python floating point numbers to tf.float32. Otherwise TensorFlow uses the same rules numpy uses when converting to arrays.
#### Evaluating Tensors
#### Printing Tensors


遇到的困惑：一开始不是很能体会shape和Dimension, 在联系Variable的函数时才更明白了一些。  
例： my_int_variable = tf.get_variable("my_int_variable", [1, 2, 3], dtype=tf.int32, initializer=tf.zeros_initializer)  
这行代码的输出结果是 [[[0 0 0] [0 0 0]]]
命令行输出的结果有点不明显，改下格式 : 【       【【0 0 0】   【0 0 0】】       】, 最外面大括号为第一个维度，第二个大括号为第二个维度，剩下的包括3个0的大括号为第三个维度

### Variables
这一章不太看得懂
#### Creating a Variable
tf.get_variable function
#### Variable collections
#### Device placement
#### Initializing variables
session.run(tf.global_variables_initializer())

### Graphs and Sessions
TensorFlow uses a dataflow graph to represent your computation in terms of the dependencies between individual operations. This leads to a low-level programming model in which you first define the dataflow graph, then create a TensorFlow session to run parts of the graph across a set of local and remote devices.


A computational graph is a series of TensorFlow operations arranged into a graph. The graph is composed of two types of objects.  
* tf.Operation (or "ops"): The nodes of the graph. Operations describe calculations that consume and produce tensors.
* tf.Tensor: The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return tf.Tensors.

**Important: tf.Tensors do not have values, they are just handles to elements in the computation graph.** 

#### Dataflow graphs
Dataflow is a common programming model for parallel computing.
#### tf.Graph
##### Graph structure
##### Graph collections
#### Dataflow graphs
#### Naming operations
The graph visualizer uses name scopes to group operations and reduce the visual complexity of a graph.
#### Placing operations on different devices
Tensorflow 可以将ops分配到不同的设备上运行
#### Tensor-like objects
#### Executing a graph in a tf.Session
#### Visualizing your graph
Tensorboard
#### Programming with multiple graphs
### Save and Restore
The tf.train.Saver class provides methods to save and restore models. The tf.saved_model.simple_save function is an easy way to build a tf.saved_model suitable for serving.
#### Save and restore variables
TensorFlow saves variables in binary checkpoint files that map variable names to tensor values.  
Saver 对象也可以自由地选择ariables进行保存和载入  
TF 提供了inspect_checkpoint 用于查看ckpt文件中的数据
#### Save and restore models
Use SavedModel to save and load your model—variables, the graph, and the graph's metadata.
##### Simple save
##### Manually build a SavedModel
The tf.saved_model.builder.SavedModelBuilder class provides functionality to save multiple MetaGraphDefs.A MetaGraph is a dataflow graph, plus its associated variables, assets, and signatures.
##### Loading a SavedModel in Python
The Python version of the SavedModel tf.saved_model.loader provides load and restore capability for a SavedModel. The load operation requires the following information:

+ The session in which to restore the graph definition and variables.
+ The tags used to identify the MetaGraphDef to load.
+ The location (directory) of the SavedModel.

Upon a load, the subset of variables, assets, and signatures supplied as part of the specific MetaGraphDef will be restored into the supplied session.
##### Load and serve a SavedModel in TensorFlow serving
##### CLI to inspect and execute SavedModel
You can use the SavedModel Command Line Interface (CLI) to inspect and execute a SavedModel. 

## ML Concepts
### Embeddings
This document introduces the concept of embeddings, gives a simple example of how to train an embedding in TensorFlow, and explains how to view embeddings with the TensorBoard Embedding Projector.  
An **embedding** is a mapping from discrete objects, such as words, to vectors of real numbers.  
Embeddings are important for input to machine learning. Classifiers, and neural networks more generally, work on vectors of real numbers. They train best on dense vectors, where all values contribute to define an object. However, many important inputs to machine learning, such as words of text, do not have a natural vector representation. Embedding functions are the standard and effective way to transform such discrete input objects into useful continuous vectors.
#### Visualizing Embeddings
TensorBoard includes the Embedding Projector, a tool that lets you interactively visualize embeddings.
