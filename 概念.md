## 回归问题

回归问题通常是用来预测一个值，如预测房价、未来的天气情况等等，例如一个产品的实际价格为500元，通过回归分析预测值为499元，我们认为这是一个比较好的回归分析。一个比较常见的回归算法是线性回归算法（LR）。另外，回归分析用在神经网络上，其最上层是不需要加上softmax函数的，而是直接对前一层累加即可。回归是对真实值的一种逼近预测。

##  分类问题

分类问题是用于将事物打上一个标签，通常结果为离散值。例如判断一幅图片上的动物是一只猫还是一只狗，分类通常是建立在回归之上，分类的最后一层通常要使用softmax函数进行判断其所属类别。分类并没有逼近的概念，最终正确结果只有一个，错误的就是错误的，不会有相近的概念。最常见的分类方法是逻辑回归，或者叫逻辑分类。

### 归一化指数函数(Softmax)

在数学，尤其是概率论和相关领域中，归一化指数函数，或称Softmax函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z“压缩”到另一个K维实向量σ(z)中，使得每一个元素的范围都在(0,1)之间，并且所有元素的和为1。该函数多用于多分类问题中。

以下是归一化指数函数代码实现示例，输入向量 [1,2,3,4,1,2,3]对应的Softmax函数的值为[0.024,0.064,0.175,0.475,0.024,0.064,0.175]。输出向量中拥有最大权重的项对应着输入向量中的最大值“4”。这也显示了这个函数通常的意义：对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。

## 损失
一种衡量指标，用于衡量模型的预测结果偏离其标签的程度。或者更直接地说是衡量模型预测的好坏。要计算此值，我们必须给模型定义损失函数。例如，线性回归模型通常将均方误差用作损失函数，而逻辑回归模型则使用对数损失函数。

### 损失函数
#### 对数损失函数（Log loss）
#### 平方损失函数（Square loss)
#### 指数损失函数（Exp loss）


# 标题未定义

**参数越多，模型越复杂，而越复杂的模型越容易过拟合。** 过拟合就是说模型在训练数据上的效果远远好于在测试集上的性能。此时可以考虑正则化，通过设置正则项前面的hyper parameter，来权衡损失函数和正则项，减小参数规模，达到模型简化的目的，从而使模型具有更好的泛化能力。
